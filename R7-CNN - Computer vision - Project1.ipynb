{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VQmz-DHAzaHS"
   },
   "source": [
    "# **Problem Statement**\n",
    "\n",
    "\n",
    "Can you differentiate a weed from a crop seedling?\n",
    "\n",
    "The ability to do so effectively can mean better crop yields and better stewardship of the environment.\n",
    "\n",
    "The Aarhus University Signal Processing group, in collaboration with University of Southern Denmark, has recently released a dataset containing images of approximately 960 unique plants belonging to 12 species at several growth stages.\n",
    "\n",
    "We're hosting this dataset as a Kaggle competition in order to give it wider exposure, to give the community an opportunity to experiment with different image recognition techniques, as well to provide a place to cross-pollenate ideas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sfYJCKvzdtr2"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import to_categorical\n",
    "from smallervggnet import SmallerVGGNet\n",
    "from imutils import paths\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1297,
     "status": "ok",
     "timestamp": 1559488870376,
     "user": {
      "displayName": "Diksha Singh",
      "photoUrl": "https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg",
      "userId": "07661533439928129655"
     },
     "user_tz": -330
    },
    "id": "aFn0Xrmxx4UD",
    "outputId": "8e5655f9-d990-445c-bf3e-741ca5d68da1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1457,
     "status": "ok",
     "timestamp": 1559488854994,
     "user": {
      "displayName": "Diksha Singh",
      "photoUrl": "https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg",
      "userId": "07661533439928129655"
     },
     "user_tz": -330
    },
    "id": "xY12XEZKxsuB",
    "outputId": "c73fdd26-6b25-4c42-f77a-ea4f5133397a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "INIT_LR = 1e-3\n",
    "BS = 32\n",
    "IMAGE_DIMS = (100, 100, 3)\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "Abs_path='content/gdrive/My Drive/Colab Notebooks/CNN/train/'\n",
    "print(\"[INFO] loading images...\")\n",
    "for imgPath in list(paths.list_images(Abs_path)):\n",
    "    # load the image, pre-process it, and store it in the data list\n",
    "    image = cv2.imread(imgPath)\n",
    "    image = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "    image = img_to_array(image)\n",
    "    data.append(image)\n",
    "    label=imgPath.split('\\\\')[-2]\n",
    "    labels.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4HG0WkVdtsS"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "(trainX, testX, trainY, testY) = train_test_split(data,labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YK4QaLDodtsm"
   },
   "outputs": [],
   "source": [
    "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "horizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] compiling model...\")\n",
    "model = SmallerVGGNet.build(width=IMAGE_DIMS[1], height=IMAGE_DIMS[0],\n",
    "depth=IMAGE_DIMS[2], classes=len(lb.classes_))\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RI0e2-5HdttA",
    "outputId": "d32cb705-70ea-48b1-9caf-8e903d23a6d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training network...\n",
      "Epoch 1/100\n",
      "118/118 [==============================] - 546s 5s/step - loss: 2.3343 - acc: 0.3633 - val_loss: 12.3454 - val_acc: 0.1474\n",
      "Epoch 2/100\n",
      "118/118 [==============================] - 543s 5s/step - loss: 1.4678 - acc: 0.5465 - val_loss: 15.1672 - val_acc: 0.0368\n",
      "Epoch 3/100\n",
      "118/118 [==============================] - 22123s 187s/step - loss: 1.1118 - acc: 0.6369 - val_loss: 13.9879 - val_acc: 0.1316\n",
      "Epoch 4/100\n",
      "118/118 [==============================] - 470s 4s/step - loss: 0.9739 - acc: 0.6922 - val_loss: 13.6957 - val_acc: 0.1474\n",
      "Epoch 5/100\n",
      "118/118 [==============================] - 548s 5s/step - loss: 0.7906 - acc: 0.7381 - val_loss: 13.9892 - val_acc: 0.1316\n",
      "Epoch 6/100\n",
      "118/118 [==============================] - 618s 5s/step - loss: 0.7378 - acc: 0.7539 - val_loss: 15.3376 - val_acc: 0.0484\n",
      "Epoch 7/100\n",
      "118/118 [==============================] - 586s 5s/step - loss: 0.8461 - acc: 0.7319 - val_loss: 15.2711 - val_acc: 0.0474\n",
      "Epoch 8/100\n",
      "118/118 [==============================] - 630s 5s/step - loss: 0.6943 - acc: 0.7786 - val_loss: 15.1195 - val_acc: 0.0484\n",
      "Epoch 9/100\n",
      "118/118 [==============================] - 649s 6s/step - loss: 0.6750 - acc: 0.7828 - val_loss: 14.7777 - val_acc: 0.0832\n",
      "Epoch 10/100\n",
      "118/118 [==============================] - 656s 6s/step - loss: 0.6214 - acc: 0.7948 - val_loss: 14.7765 - val_acc: 0.0832\n",
      "Epoch 11/100\n",
      "118/118 [==============================] - 583s 5s/step - loss: 0.5482 - acc: 0.8148 - val_loss: 14.7777 - val_acc: 0.0832\n",
      "Epoch 12/100\n",
      "118/118 [==============================] - 577s 5s/step - loss: 0.5129 - acc: 0.8233 - val_loss: 12.1833 - val_acc: 0.1474\n",
      "Epoch 13/100\n",
      "118/118 [==============================] - 1201s 10s/step - loss: 0.4837 - acc: 0.8368 - val_loss: 13.9973 - val_acc: 0.1316\n",
      "Epoch 14/100\n",
      "118/118 [==============================] - 581s 5s/step - loss: 0.4310 - acc: 0.8516 - val_loss: 11.3627 - val_acc: 0.1474\n",
      "Epoch 15/100\n",
      "  6/118 [>.............................] - ETA: 7:09 - loss: 0.4278 - acc: 0.8542"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit_generator(\n",
    "aug.flow(trainX, trainY, batch_size=BS),\n",
    "validation_data=(testX, testY),\n",
    "steps_per_epoch=len(trainX) // BS,\n",
    "epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Jbep4M_dttV"
   },
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "print(\"[INFO] serializing network...\")\n",
    "model.save('Plant_classifier.model')\n",
    " \n",
    "# save the label binarizer to disk\n",
    "print(\"[INFO] serializing label binarizer...\")\n",
    "f = open('labels.pickle', \"wb\")\n",
    "f.write(pickle.dumps(lb))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txYGx_YXdttg"
   },
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "N = EPOCHS\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.savefig(args[\"plot\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "R7-CNN - Computer vision - Project1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
