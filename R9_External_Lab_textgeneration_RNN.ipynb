{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R9_External_Lab_textgeneration_RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yllJR0PQpm4M"
      },
      "source": [
        "# Text generation using a RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2W6g03QsptwG"
      },
      "source": [
        "Given a sequence of words from this data, train a model to predict the next word in the sequence. Longer sequences of text can be generated by calling the model repeatedly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hbd3E0IuHwjz"
      },
      "source": [
        "**Mount your Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsdGgBMY0PE4",
        "colab_type": "code",
        "outputId": "a60cd0ad-e934-45e9-9a36-75936e848d61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fnIX_mLXHdxS"
      },
      "source": [
        "### Import Keras and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0TmrQuvpHdxU",
        "outputId": "1afdef15-170b-4f90-e0e2-dba4bd67cfe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import glob\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zKBXQflGlPjG"
      },
      "source": [
        "## Download data\n",
        "Reference: Data is collected from http://www.gutenberg.org\n",
        "\n",
        "For the lab purpose, you can load the dataset provided by Great Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aSNixIr3cGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/PGP-AIML/Sequence Models in NLP/Notebooks')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s321mV4DHdxZ"
      },
      "source": [
        "### Load the Oscar Wilde dataset\n",
        "\n",
        "Store all the \".txt\" file names in a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-MxOQgjmWzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile('data.zip', 'r') as z:\n",
        "  z.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VUanlzNJHdxa",
        "colab": {}
      },
      "source": [
        "oscar_wilde_list = glob.glob(\"./data/*.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6yFSUZ3mSc7",
        "colab_type": "code",
        "outputId": "415567c7-9ff8-4bee-ff12-186a7837bfc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "oscar_wilde_list"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./data/For Love of the King.txt',\n",
              " './data/Salomé A tragedy in one act.txt',\n",
              " './data/Impressions of America.txt',\n",
              " './data/The Canterville Ghost.txt',\n",
              " './data/A House of Pomegranates.txt',\n",
              " './data/Miscellaneous Aphorisms_ The Soul of Man.txt',\n",
              " './data/A Woman of No Importance a play.txt',\n",
              " './data/Essays and Lectures.txt',\n",
              " './data/The Happy Prince and other tales.txt',\n",
              " './data/Rose Leaf and Apple Leaf.txt',\n",
              " './data/Vera or, The Nihilists.txt',\n",
              " './data/Lord Arthur Savile_s Crime.txt',\n",
              " './data/Poems with the Ballad of Reading Gaol.txt',\n",
              " './data/Selected poems of oscar wilde including The Ballad of Reading Gaol.txt',\n",
              " './data/Charmides and Other Poems.txt',\n",
              " './data/An Ideal Husband.txt',\n",
              " './data/The Duchess of Padua.txt',\n",
              " './data/Oscar Wilde Miscellaneous.txt',\n",
              " './data/The Ballad of Reading Gaol.txt',\n",
              " './data/Shorter Prose Pieces.txt',\n",
              " './data/Children in Prison and Other Cruelties of Prison Life.txt',\n",
              " './data/Reviews.txt',\n",
              " './data/A Critic in Pall Mall.txt',\n",
              " './data/De Profundis.txt',\n",
              " './data/Miscellanies.txt',\n",
              " './data/The Importance of Being Earnest.txt',\n",
              " './data/Selected prose of oscar wilde with a Preface by Robert Ross.txt',\n",
              " './data/The Soul of Man.txt',\n",
              " './data/Lady Windermere_s Fan.txt',\n",
              " './data/Intentions.txt',\n",
              " './data/The Picture of Dorian Gray.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "glr4hv6uZkL-"
      },
      "source": [
        "### Read the data\n",
        "\n",
        "Read contents of every file from the list and append the text in a new list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zLrMMjrkRt9x",
        "colab": {}
      },
      "source": [
        "codetext = []\n",
        "bookranges = []\n",
        "for oscar_wilde_file in oscar_wilde_list:\n",
        "    oscar_wilde_text = open(oscar_wilde_file, \"r\")\n",
        "    start = len(codetext)\n",
        "    codetext.append(oscar_wilde_text.read())\n",
        "    end = len(codetext)\n",
        "    bookranges.append({\"start\": start, \"end\": end, \"name\": oscar_wilde_file.rsplit(\"/\", 1)[-1]})\n",
        "    oscar_wilde_text.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jMX-Fu-GHdxj"
      },
      "source": [
        "## Process the text\n",
        "Initialize and fit the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zQf1AV8wHdxl",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(lower=True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(codetext)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vpZ0A2-xHdxp"
      },
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping words to numbers, and another for numbers to words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Nsq-rSPHdxq",
        "colab": {}
      },
      "source": [
        "word_idx = tokenizer.word_index\n",
        "idx_word = tokenizer.index_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YMYdjx4aHdxu"
      },
      "source": [
        "Get the word count for every word and also get the total number of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ioEZ2c21Hdxw",
        "colab": {}
      },
      "source": [
        "#word count for every word\n",
        "word_counts = tokenizer.word_counts\n",
        "#total number of words\n",
        "num_words = len(word_idx) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dWUBr9rHHdx0"
      },
      "source": [
        "Convert text to sequence of numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dwLl0BWKHdx2",
        "colab": {}
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(codetext)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GkpK8McUHdx6"
      },
      "source": [
        "### Generate Features and Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zxhQamjwHdx7",
        "colab": {}
      },
      "source": [
        "features = []\n",
        "labels = []\n",
        "\n",
        "training_length = 50\n",
        "# Iterate through the sequences of tokens\n",
        "for seq in sequences:\n",
        "    # Create multiple training examples from each sequence\n",
        "    for i in range(training_length, training_length+300):\n",
        "        # Extract the features and label\n",
        "        extract = seq[i - training_length: i - training_length + 20]\n",
        "\n",
        "        # Set the features and label\n",
        "        features.append(extract[:-1])\n",
        "        labels.append(extract[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### The prediction task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wssHQ1oGymwe"
      },
      "source": [
        "Given a word, or a sequence of words, what is the most probable next word? This is the task we're training the model to perform. The input to the model will be a sequence of words, and we train the model to predict the output—the following word at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the words computed until this moment, what is the next word?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T2bsVOl7HdyA"
      },
      "source": [
        "### Generate training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j7-IsvynHdyB",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "\n",
        "features, labels = shuffle(features, labels, random_state=1)\n",
        "\n",
        "# Decide on number of samples for training\n",
        "train_end = int(0.7 * len(labels))\n",
        "\n",
        "train_features = np.array(features[:train_end])\n",
        "valid_features = np.array(features[train_end:])\n",
        "\n",
        "train_labels = labels[:train_end]\n",
        "valid_labels = labels[train_end:]\n",
        "\n",
        "# Convert to arrays\n",
        "X_train, X_valid = np.array(train_features), np.array(valid_features)\n",
        "\n",
        "# Using int8 for memory savings\n",
        "y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n",
        "y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n",
        "\n",
        "# One hot encoding of labels\n",
        "for example_index, word_index in enumerate(train_labels):\n",
        "    y_train[example_index, word_index] = 1\n",
        "\n",
        "for example_index, word_index in enumerate(valid_labels):\n",
        "    y_valid[example_index, word_index] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "juT1mZrUHdyE"
      },
      "source": [
        "This is just to check the features and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wkdmNbgjHdyF",
        "outputId": "6c39ce15-3650-4045-9a0d-0dc36873d258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "for i, sequence in enumerate(X_train[:2]):\n",
        "    text = []\n",
        "#     print(i, sequence)\n",
        "    for idx in sequence:\n",
        "        text.append(idx_word[idx])\n",
        "        \n",
        "    print('Features: ' + ' '.join(text)+'\\n')\n",
        "    print('Label: ' + idx_word[np.argmax(y_train[i])] + '\\n')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features: room in algernon's flat in half moon street the room is luxuriously and artistically furnished the sound of a\n",
            "\n",
            "Label: piano\n",
            "\n",
            "Features: a woman of no importance a play author oscar wilde release date september 16 2014 ebook 854 this file\n",
            "\n",
            "Label: was\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "Use `keras.Sequential` to define the model. For this simple example three layers are used to define our model:\n",
        "\n",
        "* `keras.layers.Embedding`: The input layer. A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n",
        "* `keras.layers.LSTM`: A type of RNN with size `units=rnn_units` (You can also use a GRU layer here.)\n",
        "* `keras.layers.Dense`: The output layer, with `num_words` outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GKpCQFZLHdyN",
        "outputId": "20ffb92a-4e69-4c9e-9b68-4ec4171ee84f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Embedding layer\n",
        "model.add(\n",
        "    Embedding(\n",
        "        input_dim=len(word_idx) + 1,\n",
        "        output_dim=100,\n",
        "        weights=None,\n",
        "        trainable=True))\n",
        "\n",
        "# Recurrent layer\n",
        "model.add(\n",
        "    LSTM(\n",
        "        64, return_sequences=False, dropout=0.1,\n",
        "        recurrent_dropout=0.1))\n",
        "\n",
        "# Fully connected layer\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "# Dropout for regularization\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(num_words, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0717 11:39:44.489612 140614241126272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0717 11:39:44.528427 140614241126272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0717 11:39:44.537681 140614241126272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0717 11:39:44.678330 140614241126272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0717 11:39:44.692908 140614241126272 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0717 11:39:45.135169 140614241126272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0717 11:39:45.160378 140614241126272 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 100)         3283900   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 64)                42240     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32839)             2134535   \n",
            "=================================================================\n",
            "Total params: 5,464,835\n",
            "Trainable params: 5,464,835\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vL3tUp1UHdyS"
      },
      "source": [
        "For each word the model looks up the embedding, runs the LSTM one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-liklihood of the next word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6o84puBcHdyV",
        "outputId": "be83d07b-0ab2-4aae-ed69-1da5904afda7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "h = model.fit(X_train, y_train, epochs = 200, batch_size = 50, \n",
        "          verbose = 1)## Train the model"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0717 11:39:45.323875 140614241126272 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "6510/6510 [==============================] - 22s 3ms/step - loss: 8.2417 - acc: 0.0570\n",
            "Epoch 2/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 6.7221 - acc: 0.0616\n",
            "Epoch 3/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 6.4449 - acc: 0.0608\n",
            "Epoch 4/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 6.2761 - acc: 0.0602\n",
            "Epoch 5/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 6.1197 - acc: 0.0648\n",
            "Epoch 6/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 6.0020 - acc: 0.0677\n",
            "Epoch 7/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 5.8519 - acc: 0.0693\n",
            "Epoch 8/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 5.7305 - acc: 0.0763\n",
            "Epoch 9/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 5.6324 - acc: 0.0811\n",
            "Epoch 10/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 5.5425 - acc: 0.0810\n",
            "Epoch 11/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 5.4350 - acc: 0.0899\n",
            "Epoch 12/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 5.3023 - acc: 0.1048\n",
            "Epoch 13/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 5.1636 - acc: 0.1192\n",
            "Epoch 14/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 5.0250 - acc: 0.1350\n",
            "Epoch 15/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 4.9075 - acc: 0.1409\n",
            "Epoch 16/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 4.7568 - acc: 0.1624\n",
            "Epoch 17/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 4.6340 - acc: 0.1776\n",
            "Epoch 18/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 4.4933 - acc: 0.1963\n",
            "Epoch 19/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 4.3736 - acc: 0.2140\n",
            "Epoch 20/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 4.2513 - acc: 0.2349\n",
            "Epoch 21/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 4.1365 - acc: 0.2472\n",
            "Epoch 22/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 4.0200 - acc: 0.2654\n",
            "Epoch 23/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 3.9163 - acc: 0.2774\n",
            "Epoch 24/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 3.7964 - acc: 0.2942\n",
            "Epoch 25/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 3.7007 - acc: 0.3061\n",
            "Epoch 26/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 3.6002 - acc: 0.3174\n",
            "Epoch 27/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 3.5059 - acc: 0.3284\n",
            "Epoch 28/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 3.4208 - acc: 0.3429\n",
            "Epoch 29/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 3.3462 - acc: 0.3507\n",
            "Epoch 30/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 3.2750 - acc: 0.3604\n",
            "Epoch 31/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 3.1875 - acc: 0.3670\n",
            "Epoch 32/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 3.1142 - acc: 0.3808\n",
            "Epoch 33/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 3.0533 - acc: 0.3888\n",
            "Epoch 34/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 3.0016 - acc: 0.3911\n",
            "Epoch 35/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 2.9249 - acc: 0.4080\n",
            "Epoch 36/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 2.8602 - acc: 0.4103\n",
            "Epoch 37/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 2.7891 - acc: 0.4169\n",
            "Epoch 38/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 2.7199 - acc: 0.4284\n",
            "Epoch 39/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 2.6673 - acc: 0.4361\n",
            "Epoch 40/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 2.6021 - acc: 0.4442\n",
            "Epoch 41/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 2.5598 - acc: 0.4464\n",
            "Epoch 42/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 2.5097 - acc: 0.4544\n",
            "Epoch 43/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 2.4379 - acc: 0.4688\n",
            "Epoch 44/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 2.4117 - acc: 0.4593\n",
            "Epoch 45/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 2.3428 - acc: 0.4745\n",
            "Epoch 46/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 2.3107 - acc: 0.4777\n",
            "Epoch 47/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 2.2424 - acc: 0.4885\n",
            "Epoch 48/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 2.2209 - acc: 0.4873\n",
            "Epoch 49/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 2.2018 - acc: 0.4988\n",
            "Epoch 50/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 2.1575 - acc: 0.4909\n",
            "Epoch 51/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 2.0988 - acc: 0.5058\n",
            "Epoch 52/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 2.0676 - acc: 0.5154\n",
            "Epoch 53/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 2.0447 - acc: 0.5141\n",
            "Epoch 54/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.9881 - acc: 0.5209\n",
            "Epoch 55/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.9374 - acc: 0.5349\n",
            "Epoch 56/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.9077 - acc: 0.5366\n",
            "Epoch 57/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.8974 - acc: 0.5324\n",
            "Epoch 58/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.8373 - acc: 0.5467\n",
            "Epoch 59/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.8140 - acc: 0.5527\n",
            "Epoch 60/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.7881 - acc: 0.5624\n",
            "Epoch 61/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.7334 - acc: 0.5720\n",
            "Epoch 62/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.7190 - acc: 0.5693\n",
            "Epoch 63/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 1.7228 - acc: 0.5602\n",
            "Epoch 64/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.6824 - acc: 0.5724\n",
            "Epoch 65/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 1.6491 - acc: 0.5803\n",
            "Epoch 66/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.6167 - acc: 0.5888\n",
            "Epoch 67/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.5967 - acc: 0.5972\n",
            "Epoch 68/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.5671 - acc: 0.5977\n",
            "Epoch 69/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.5654 - acc: 0.5983\n",
            "Epoch 70/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.5182 - acc: 0.6034\n",
            "Epoch 71/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.5120 - acc: 0.6075\n",
            "Epoch 72/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.4877 - acc: 0.6111\n",
            "Epoch 73/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.4495 - acc: 0.6154\n",
            "Epoch 74/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.4284 - acc: 0.6229\n",
            "Epoch 75/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.4270 - acc: 0.6212\n",
            "Epoch 76/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.4127 - acc: 0.6258\n",
            "Epoch 77/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.3954 - acc: 0.6306\n",
            "Epoch 78/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.3551 - acc: 0.6456\n",
            "Epoch 79/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.3379 - acc: 0.6462\n",
            "Epoch 80/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.3445 - acc: 0.6375\n",
            "Epoch 81/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.3235 - acc: 0.6456\n",
            "Epoch 82/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.3112 - acc: 0.6450\n",
            "Epoch 83/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.2777 - acc: 0.6528\n",
            "Epoch 84/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.2797 - acc: 0.6519\n",
            "Epoch 85/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.2833 - acc: 0.6518\n",
            "Epoch 86/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.2547 - acc: 0.6555\n",
            "Epoch 87/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.2301 - acc: 0.6697\n",
            "Epoch 88/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.2270 - acc: 0.6636\n",
            "Epoch 89/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.1967 - acc: 0.6720\n",
            "Epoch 90/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.1815 - acc: 0.6728\n",
            "Epoch 91/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.1724 - acc: 0.6737\n",
            "Epoch 92/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.1540 - acc: 0.6820\n",
            "Epoch 93/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.1395 - acc: 0.6828\n",
            "Epoch 94/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.1452 - acc: 0.6825\n",
            "Epoch 95/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.1324 - acc: 0.6820\n",
            "Epoch 96/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.0946 - acc: 0.6995\n",
            "Epoch 97/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.1118 - acc: 0.6912\n",
            "Epoch 98/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.0949 - acc: 0.6869\n",
            "Epoch 99/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.0939 - acc: 0.6945\n",
            "Epoch 100/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.0625 - acc: 0.7005\n",
            "Epoch 101/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.0431 - acc: 0.7054\n",
            "Epoch 102/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.0435 - acc: 0.7022\n",
            "Epoch 103/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.0552 - acc: 0.6959\n",
            "Epoch 104/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.0174 - acc: 0.7163\n",
            "Epoch 105/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.0105 - acc: 0.7121\n",
            "Epoch 106/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.0066 - acc: 0.7172\n",
            "Epoch 107/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.0018 - acc: 0.7135\n",
            "Epoch 108/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 1.0107 - acc: 0.7177\n",
            "Epoch 109/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.9931 - acc: 0.7209\n",
            "Epoch 110/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.9879 - acc: 0.7189\n",
            "Epoch 111/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.9641 - acc: 0.7217\n",
            "Epoch 112/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.9583 - acc: 0.7295\n",
            "Epoch 113/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.9610 - acc: 0.7250\n",
            "Epoch 114/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.9440 - acc: 0.7247\n",
            "Epoch 115/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.9321 - acc: 0.7346\n",
            "Epoch 116/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.9282 - acc: 0.7321\n",
            "Epoch 117/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.9387 - acc: 0.7237\n",
            "Epoch 118/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.9164 - acc: 0.7359\n",
            "Epoch 119/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.8982 - acc: 0.7409\n",
            "Epoch 120/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.9020 - acc: 0.7382\n",
            "Epoch 121/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.8741 - acc: 0.7445\n",
            "Epoch 122/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.8994 - acc: 0.7292\n",
            "Epoch 123/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.8803 - acc: 0.7478\n",
            "Epoch 124/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.8567 - acc: 0.7553\n",
            "Epoch 125/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.8600 - acc: 0.7487\n",
            "Epoch 126/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.8659 - acc: 0.7504\n",
            "Epoch 127/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.8492 - acc: 0.7473\n",
            "Epoch 128/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.8506 - acc: 0.7559\n",
            "Epoch 129/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.8707 - acc: 0.7461\n",
            "Epoch 130/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.8405 - acc: 0.7528\n",
            "Epoch 131/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.8403 - acc: 0.7499\n",
            "Epoch 132/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.8059 - acc: 0.7585\n",
            "Epoch 133/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.8325 - acc: 0.7555\n",
            "Epoch 134/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.8151 - acc: 0.7588\n",
            "Epoch 135/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.8159 - acc: 0.7604\n",
            "Epoch 136/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.7879 - acc: 0.7661\n",
            "Epoch 137/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.8053 - acc: 0.7613\n",
            "Epoch 138/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.8100 - acc: 0.7570\n",
            "Epoch 139/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.8000 - acc: 0.7651\n",
            "Epoch 140/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.7718 - acc: 0.7725\n",
            "Epoch 141/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.7900 - acc: 0.7685\n",
            "Epoch 142/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.7775 - acc: 0.7730\n",
            "Epoch 143/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.7552 - acc: 0.7779\n",
            "Epoch 144/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.7679 - acc: 0.7696\n",
            "Epoch 145/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.7615 - acc: 0.7730\n",
            "Epoch 146/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.7695 - acc: 0.7762\n",
            "Epoch 147/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.7426 - acc: 0.7762\n",
            "Epoch 148/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.7451 - acc: 0.7817\n",
            "Epoch 149/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.7616 - acc: 0.7714\n",
            "Epoch 150/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.7458 - acc: 0.7768\n",
            "Epoch 151/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.7350 - acc: 0.7805\n",
            "Epoch 152/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.7275 - acc: 0.7843\n",
            "Epoch 153/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.7329 - acc: 0.7799\n",
            "Epoch 154/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.7424 - acc: 0.7742\n",
            "Epoch 155/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.7015 - acc: 0.7917\n",
            "Epoch 156/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6804 - acc: 0.7937\n",
            "Epoch 157/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.7129 - acc: 0.7846\n",
            "Epoch 158/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.7070 - acc: 0.7906\n",
            "Epoch 159/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.7010 - acc: 0.7914\n",
            "Epoch 160/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.6856 - acc: 0.7965\n",
            "Epoch 161/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.6814 - acc: 0.7897\n",
            "Epoch 162/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.6947 - acc: 0.7889\n",
            "Epoch 163/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.6835 - acc: 0.7966\n",
            "Epoch 164/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.6885 - acc: 0.7932\n",
            "Epoch 165/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.6856 - acc: 0.7977\n",
            "Epoch 166/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.6657 - acc: 0.7965\n",
            "Epoch 167/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.6792 - acc: 0.7959\n",
            "Epoch 168/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.6682 - acc: 0.7992\n",
            "Epoch 169/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.6603 - acc: 0.8025\n",
            "Epoch 170/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.6701 - acc: 0.7968\n",
            "Epoch 171/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.6766 - acc: 0.7943\n",
            "Epoch 172/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6557 - acc: 0.8029\n",
            "Epoch 173/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.6605 - acc: 0.8005\n",
            "Epoch 174/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6606 - acc: 0.8014\n",
            "Epoch 175/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6484 - acc: 0.7997\n",
            "Epoch 176/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6520 - acc: 0.8026\n",
            "Epoch 177/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6626 - acc: 0.7991\n",
            "Epoch 178/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6431 - acc: 0.8011\n",
            "Epoch 179/200\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.6416 - acc: 0.8089\n",
            "Epoch 180/200\n",
            "6510/6510 [==============================] - 24s 4ms/step - loss: 0.6213 - acc: 0.8060\n",
            "Epoch 181/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6363 - acc: 0.8086\n",
            "Epoch 182/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6542 - acc: 0.7986\n",
            "Epoch 183/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6025 - acc: 0.8184\n",
            "Epoch 184/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6200 - acc: 0.8078\n",
            "Epoch 185/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6288 - acc: 0.8126\n",
            "Epoch 186/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6368 - acc: 0.8098\n",
            "Epoch 187/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6146 - acc: 0.8114\n",
            "Epoch 188/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6378 - acc: 0.8032\n",
            "Epoch 189/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.5972 - acc: 0.8169\n",
            "Epoch 190/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6123 - acc: 0.8095\n",
            "Epoch 191/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6155 - acc: 0.8134\n",
            "Epoch 192/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6124 - acc: 0.8158\n",
            "Epoch 193/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6199 - acc: 0.8108\n",
            "Epoch 194/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6006 - acc: 0.8127\n",
            "Epoch 195/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.5924 - acc: 0.8171\n",
            "Epoch 196/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6156 - acc: 0.8132\n",
            "Epoch 197/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.5862 - acc: 0.8184\n",
            "Epoch 198/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.5795 - acc: 0.8169\n",
            "Epoch 199/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.6278 - acc: 0.8051\n",
            "Epoch 200/200\n",
            "6510/6510 [==============================] - 21s 3ms/step - loss: 0.5833 - acc: 0.8204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "82716QWAJrXG"
      },
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_8MFkQgYJm-D",
        "colab": {}
      },
      "source": [
        "# save the model to file\n",
        "model.save('./data/model_oscar_wilde.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4AYVKydVJv5C"
      },
      "source": [
        "## If you have already trained the model and saved it, you can load a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IqsQUz04J0GP",
        "colab": {}
      },
      "source": [
        "# load the model\n",
        "model = load_model('./data/model_oscar_wilde.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZFe2Y0SJJ3Hb"
      },
      "source": [
        "### Note: After loading the model run  model.fit()  to continue training form there, if required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e9yLm_xnJ5JV",
        "outputId": "56a47017-9d25-41d6-89f0-6dd4029ff83c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=50, epochs=15)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.5840 - acc: 0.8189\n",
            "Epoch 2/15\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.5886 - acc: 0.8226\n",
            "Epoch 3/15\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.5676 - acc: 0.8273\n",
            "Epoch 4/15\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.5852 - acc: 0.8210\n",
            "Epoch 5/15\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.5862 - acc: 0.8177\n",
            "Epoch 6/15\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.5840 - acc: 0.8224\n",
            "Epoch 7/15\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.5665 - acc: 0.8218\n",
            "Epoch 8/15\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.5802 - acc: 0.8210\n",
            "Epoch 9/15\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.5626 - acc: 0.8273\n",
            "Epoch 10/15\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.5462 - acc: 0.8346\n",
            "Epoch 11/15\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.5775 - acc: 0.8221\n",
            "Epoch 12/15\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.5502 - acc: 0.8300\n",
            "Epoch 13/15\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.5576 - acc: 0.8298\n",
            "Epoch 14/15\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.5636 - acc: 0.8264\n",
            "Epoch 15/15\n",
            "6510/6510 [==============================] - 20s 3ms/step - loss: 0.5643 - acc: 0.8240\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe3121644e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EmkaxXdjHdyd"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7RraFX9YHdye",
        "outputId": "87b5a32a-9657-4bc4-9b79-8149f3197b75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "print(model.evaluate(X_train, y_train, batch_size = 20))\n",
        "print('\\nModel Performance: Log Loss and Accuracy on validation data')\n",
        "print(model.evaluate(X_valid, y_valid, batch_size = 20))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6510/6510 [==============================] - 6s 976us/step\n",
            "[0.06200566711998652, 0.9847926231023902]\n",
            "\n",
            "Model Performance: Log Loss and Accuracy on validation data\n",
            "2790/2790 [==============================] - 3s 927us/step\n",
            "[8.325727577277837, 0.37526881844339405]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u5CKxykLHdyj"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4JSW5EwKHdyk",
        "outputId": "0dddfa92-b672-42d0-8e8f-8e9876e1fa70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "seed_length=50\n",
        "new_words=50\n",
        "diversity=1\n",
        "n_gen=1\n",
        "\n",
        "import random\n",
        "\n",
        "# Choose a random sequence\n",
        "seq = random.choice(sequences)\n",
        "\n",
        "# print seq\n",
        "\n",
        "# Choose a random starting point\n",
        "seed_idx = random.randint(0, len(seq) - seed_length - 10)\n",
        "# Ending index for seed\n",
        "end_idx = seed_idx + seed_length\n",
        "\n",
        "gen_list = []\n",
        "\n",
        "for n in range(n_gen):\n",
        "    # Extract the seed sequence\n",
        "    seed = seq[seed_idx:end_idx]\n",
        "    original_sequence = [idx_word[i] for i in seed]\n",
        "    generated = seed[:] + ['#']\n",
        "\n",
        "    # Find the actual entire sequence\n",
        "    actual = generated[:] + seq[end_idx:end_idx + new_words]\n",
        "        \n",
        "    # Keep adding new words\n",
        "    for i in range(new_words):\n",
        "\n",
        "        # Make a prediction from the seed\n",
        "        preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(np.float64)\n",
        "\n",
        "        # Diversify\n",
        "        preds = np.log(preds) / diversity\n",
        "        exp_preds = np.exp(preds)\n",
        "\n",
        "        # Softmax\n",
        "        preds = exp_preds / sum(exp_preds)\n",
        "\n",
        "        # Choose the next word\n",
        "        probas = np.random.multinomial(1, preds, 1)[0]\n",
        "\n",
        "        next_idx = np.argmax(probas)\n",
        "\n",
        "        # New seed adds on old word\n",
        "        #             seed = seed[1:] + [next_idx]\n",
        "        seed += [next_idx]\n",
        "        generated.append(next_idx)\n",
        "    # Showing generated and actual abstract\n",
        "    n = []\n",
        "\n",
        "    for i in generated:\n",
        "        n.append(idx_word.get(i, '< --- >'))\n",
        "\n",
        "    gen_list.append(n)\n",
        "\n",
        "a = []\n",
        "\n",
        "for i in actual:\n",
        "    a.append(idx_word.get(i, '< --- >'))\n",
        "\n",
        "a = a[seed_length:]\n",
        "\n",
        "gen_list = [gen[seed_length:seed_length + len(a)] for gen in gen_list]\n",
        "\n",
        "print('Original Sequence: \\n'+' '.join(original_sequence))\n",
        "print(\"\\n\")\n",
        "# print(gen_list)\n",
        "print('Generated Sequence: \\n'+' '.join(gen_list[0][1:]))\n",
        "# print(a)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original Sequence: \n",
            "pointed out because the poems had already been published in london and as he held the copyright they could not be reissued save with his consent furthermore since i have read the introduction i am not over pleased at the way in which i find myself identified with much that\n",
            "\n",
            "\n",
            "Generated Sequence: \n",
            "one sure from the two bright sapphires and transcribed from the french and with the disciple life the plagiarist the indispensable east and this header and am v with the text persons in the prologue peter not those who so patiently assisted him the proof readers amongst the trial of\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9aXeiz7ZHdyp",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}