{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"R8_Internal_Lab_Questions_Transfer_Learning_MNIST_Tweet_Sentiment_Analysis.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NFfDTfhlaEI_"},"source":["# Transfer Learning MNIST"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rNwbqCFRaEJC"},"source":["* Train a simple convnet on the MNIST dataset the first 5 digits [0..4].\n","* Freeze convolutional layers and fine-tune dense layers for the classification of digits [5..9]."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YUB1uDW_8XIy"},"source":["## 1. Import necessary libraries for the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Rsj4t5HTaEJE","colab":{}},"source":["#Importing important modules\n","import keras\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten,Activation, Reshape\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.datasets import mnist"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IXrn3heBaEJa"},"source":["## 2. Import MNIST data and create 2 datasets with one dataset having digits from 0 to 4 and other from 5 to 9 "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pjDuiK6ztgOK","colab":{}},"source":["(x_train, y_train), (x_test, y_test) = mnist.load_data()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XXIR8kG8dBYt","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","train_filter = np.where(y_train <= 4 )\n","test_filter = np.where(y_test<= 4 )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BZTz3AwJd7CN","colab_type":"code","colab":{}},"source":["#dataset 1 having digits from 0 to 4\n","x_train_1, y_train_1= x_train[train_filter], y_train[train_filter]\n","x_test_1, y_test_1 = x_test[test_filter], y_test[test_filter]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9RTrekU8eduf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"b176ef8d-6118-4405-b299-8eff1c9578ac","executionInfo":{"status":"ok","timestamp":1560684860273,"user_tz":-330,"elapsed":572,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["print(x_train_1.shape)\n","print(y_train_1.shape)\n","print(x_test_1.shape)\n","print(y_test_1.shape)"],"execution_count":96,"outputs":[{"output_type":"stream","text":["(30596, 28, 28)\n","(30596,)\n","(5139, 28, 28)\n","(5139,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-vXk9MrweMxF","colab_type":"code","colab":{}},"source":["train_filter2 = np.where(y_train >= 5 )\n","test_filter2 = np.where(y_test>= 5 )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gv-531waeVSc","colab_type":"code","colab":{}},"source":["#dataset 2 having digits from 5 to 9\n","x_train_2, y_train_2= x_train[train_filter2], y_train[train_filter2]\n","x_test_2, y_test_2 = x_test[test_filter2], y_test[test_filter2]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nUEEVABaezRe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"437308ac-6c07-4a13-dec8-0d8199b6bdac","executionInfo":{"status":"ok","timestamp":1560684864049,"user_tz":-330,"elapsed":966,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["print(x_train_2.shape)\n","print(y_train_2.shape)\n","print(x_test_2.shape)\n","print(y_test_2.shape)"],"execution_count":99,"outputs":[{"output_type":"stream","text":["(29404, 28, 28)\n","(29404,)\n","(4861, 28, 28)\n","(4861,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9qU14lYL9A5g"},"source":["## 3. Print x_train, y_train, x_test and y_test for both the datasets"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Z9OrszhJ0SgJ","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"74f3ce0f-4b2f-4874-85e9-7765436045f1","executionInfo":{"status":"ok","timestamp":1560684976956,"user_tz":-330,"elapsed":944,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["print(x_train_1.shape)\n","print(y_train_1.shape)\n","print(x_test_1.shape)\n","print(y_test_1.shape)"],"execution_count":100,"outputs":[{"output_type":"stream","text":["(30596, 28, 28)\n","(30596,)\n","(5139, 28, 28)\n","(5139,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sJswV4xk9jQS","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"43544b82-ccc9-423d-8b6e-c556a0711730","executionInfo":{"status":"ok","timestamp":1560684979780,"user_tz":-330,"elapsed":969,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["print(x_train_2.shape)\n","print(y_train_2.shape)\n","print(x_test_2.shape)\n","print(y_test_2.shape)"],"execution_count":101,"outputs":[{"output_type":"stream","text":["(29404, 28, 28)\n","(29404,)\n","(4861, 28, 28)\n","(4861,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cB9BPFzr9oDF"},"source":["## ** 4. Let us take only the dataset (x_train, y_train, x_test, y_test) for Integers 0 to 4 in MNIST **\n","## Reshape x_train and x_test to a 4 Dimensional array (channel = 1) to pass it into a Conv2D layer"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FlQRPfFzaEJx","colab":{}},"source":["x_train_1 = x_train_1.reshape(x_train_1.shape[0], 28, 28, 1).astype('float32')\n","x_test_1 = x_test_1.reshape(x_test_1.shape[0], 28, 28, 1).astype('float32')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jLQr-b3F-hw8"},"source":["## 5. Normalize x_train and x_test by dividing it by 255"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PlEZIAG5-g2I","colab":{}},"source":["x_train_1 /= 255\n","x_test_1 /= 255"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pytVBaw4-vMi"},"source":["## 6. Use One-hot encoding to divide y_train and y_test into required no of output classes"]},{"cell_type":"code","metadata":{"id":"co7EaCmssecV","colab_type":"code","colab":{}},"source":["from keras.utils import np_utils"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"V48xiua4-uUi","colab":{}},"source":["y_train1_cat = np_utils.to_categorical(y_train_1, 5) #converts to 10 categorical variable\n","y_test1_cat = np_utils.to_categorical(y_test_1, 5)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"elPkI44g_C2b"},"source":["## 7. Build a sequential model with 2 Convolutional layers with 32 kernels of size (3,3) followed by a Max pooling layer of size (2,2) followed by a drop out layer to be trained for classification of digits 0-4  "]},{"cell_type":"code","metadata":{"id":"fPD2FqCosHNY","colab_type":"code","colab":{}},"source":["BATCH_SIZE = 32\n","EPOCHS = 10"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MU09mm9F89gO","colab":{}},"source":["    # Define Model\n","    model = Sequential()\n","\n","    #Add a Convolutional Layer with 32 filters of size 3X3 and activation function as 'ReLU' \n","    model.add(Conv2D(32, kernel_size=(3, 3),\n","                 activation='relu',\n","                 input_shape=(28,28,1),name='conv_1'))\n","\n","    #Add a Convolutional Layer with 64 filters of size 3X3 and activation function as 'ReLU' \n","    model.add(Conv2D(32, (3, 3), activation='relu',name='conv_2'))\n","    # Max Pooling\n","    model.add(MaxPooling2D(pool_size=(2,2)))\n","    \n","    # Dropout\n","    model.add(Dropout(0.25))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sJQaycRO_3Au"},"source":["## 8. Post that flatten the data and add 2 Dense layers with 128 neurons and neurons = output classes with activation = 'relu' and 'softmax' respectively. Add dropout layer inbetween if necessary  "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vOZeRbK7t9AT","colab":{"base_uri":"https://localhost:8080/","height":403},"outputId":"a3143496-8348-44be-87a6-06018b7b761c","executionInfo":{"status":"ok","timestamp":1560685105549,"user_tz":-330,"elapsed":97080,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["    # Fully Connected Layer\n","    model.add(Flatten())\n","    model.add(Dense(128))\n","    model.add(Activation('relu'))\n","    \n","    # More Dropout\n","    model.add(Dropout(0.5))\n","\n","    # Prediction Layer\n","    model.add(Dense(5))\n","    model.add(Activation('softmax'))\n","    \n","     # Loss and Optimizer\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    \n","    # Store Training Results\n","    early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=7, verbose=1, mode='auto')\n","    callback_list = [early_stopping]\n","\n","    # Train the model\n","    model.fit(x_train_1, y_train1_cat, batch_size=BATCH_SIZE, nb_epoch=EPOCHS, \n","              validation_data=(x_test_1, y_test1_cat), callbacks=callback_list)"],"execution_count":108,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 30596 samples, validate on 5139 samples\n","Epoch 1/10\n","30596/30596 [==============================] - 10s 325us/step - loss: 0.0885 - acc: 0.9733 - val_loss: 0.0099 - val_acc: 0.9961\n","Epoch 2/10\n","30596/30596 [==============================] - 10s 314us/step - loss: 0.0289 - acc: 0.9906 - val_loss: 0.0087 - val_acc: 0.9971\n","Epoch 3/10\n","30596/30596 [==============================] - 10s 311us/step - loss: 0.0199 - acc: 0.9937 - val_loss: 0.0106 - val_acc: 0.9957\n","Epoch 4/10\n","30596/30596 [==============================] - 9s 310us/step - loss: 0.0155 - acc: 0.9952 - val_loss: 0.0074 - val_acc: 0.9973\n","Epoch 5/10\n","30596/30596 [==============================] - 9s 310us/step - loss: 0.0124 - acc: 0.9958 - val_loss: 0.0098 - val_acc: 0.9975\n","Epoch 6/10\n","30596/30596 [==============================] - 10s 311us/step - loss: 0.0137 - acc: 0.9956 - val_loss: 0.0049 - val_acc: 0.9984\n","Epoch 7/10\n","30596/30596 [==============================] - 10s 311us/step - loss: 0.0098 - acc: 0.9969 - val_loss: 0.0077 - val_acc: 0.9963\n","Epoch 8/10\n","30596/30596 [==============================] - 10s 311us/step - loss: 0.0097 - acc: 0.9969 - val_loss: 0.0051 - val_acc: 0.9984\n","Epoch 9/10\n","30596/30596 [==============================] - 10s 313us/step - loss: 0.0074 - acc: 0.9975 - val_loss: 0.0048 - val_acc: 0.9979\n","Epoch 10/10\n","30596/30596 [==============================] - 10s 312us/step - loss: 0.0089 - acc: 0.9970 - val_loss: 0.0057 - val_acc: 0.9981\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f7fb860bd30>"]},"metadata":{"tags":[]},"execution_count":108}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"my1P09bxAv8H"},"source":["## 9. Print the training and test accuracy"]},{"cell_type":"code","metadata":{"id":"kgqwE7RklLPr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"e0c923cb-b228-4d26-abc1-261240856b7e","executionInfo":{"status":"ok","timestamp":1560685119304,"user_tz":-330,"elapsed":3701,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["train_loss, train_accuracy = model.evaluate(x_train_1, y_train1_cat)\n","print(\"Train Accuracy :\",train_accuracy*100,'%')"],"execution_count":109,"outputs":[{"output_type":"stream","text":["30596/30596 [==============================] - 3s 88us/step\n","Train Accuracy : 99.96731598901818 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yf7F8Gdutbf0","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"ad3b5c9f-8833-4950-bd00-de04eb2e8c9f","executionInfo":{"status":"ok","timestamp":1560685121697,"user_tz":-330,"elapsed":1242,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["test_loss, test_accuracy = model.evaluate(x_test_1, y_test1_cat)\n","print(\"Test Accuracy :\",test_accuracy*100,'%')"],"execution_count":110,"outputs":[{"output_type":"stream","text":["5139/5139 [==============================] - 1s 99us/step\n","Test Accuracy : 99.80540961276513 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"z78o3WIjaEJ3"},"source":["## 10. Make only the dense layers to be trainable and convolutional layers to be non-trainable"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"brN7VZHFaEJ4","colab":{"base_uri":"https://localhost:8080/","height":353},"outputId":"09c8f27f-33bb-4475-c3c0-250ec5484153","executionInfo":{"status":"ok","timestamp":1560685127198,"user_tz":-330,"elapsed":898,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["#Freezing layers in the model which don't have 'dense' in their name\n","for layer in model.layers:\n","  if('dense' not in layer.name): #prefix detection to freeze layers which does not have dense\n","    #Freezing a layer\n","    layer.trainable = False\n","    \n","#Module to print colourful statements\n","from termcolor import colored\n","\n","#Check which layers have been frozen \n","for layer in model.layers:\n","  print (colored(layer.name, 'blue'))\n","  print (colored(layer.trainable, 'red'))"],"execution_count":111,"outputs":[{"output_type":"stream","text":["\u001b[34mconv_1\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mconv_2\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mmax_pooling2d_4\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mdropout_6\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mflatten_3\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mdense_5\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mactivation_7\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mdropout_7\u001b[0m\n","\u001b[31mFalse\u001b[0m\n","\u001b[34mdense_6\u001b[0m\n","\u001b[31mTrue\u001b[0m\n","\u001b[34mactivation_8\u001b[0m\n","\u001b[31mFalse\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4opnW7o0BJ8P"},"source":["## 11. Use the model trained on 0 to 4 digit classification and train it on the dataset which has digits 5 to 9  (Using Transfer learning keeping only the dense layers to be trainable)"]},{"cell_type":"code","metadata":{"id":"P2DHgPt8qXnI","colab_type":"code","colab":{}},"source":["x_train_2 = x_train_2.reshape(x_train_2.shape[0], 28, 28, 1).astype('float32')\n","x_test_2 = x_test_2.reshape(x_test_2.shape[0], 28, 28, 1).astype('float32')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"futUmim7qcHR","colab_type":"code","colab":{}},"source":["x_train_2 /= 255\n","x_test_2 /= 255"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NHZQINYXw2gb","colab_type":"code","colab":{}},"source":["y_train2_cat = np_utils.to_categorical(y_train_2, 10)\n","y_test2_cat = np_utils.to_categorical(y_test_2, 10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_FHEU1RFfZb","colab_type":"code","colab":{}},"source":["#considering only the last 5 columns\n","y_train2_cat = y_train2_cat[:,5:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"naNSFuBmGdHa","colab_type":"code","colab":{}},"source":["y_test2_cat = y_test2_cat[:,5:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8PcfeVjgHe47","colab_type":"code","colab":{}},"source":["from keras.optimizers import Adam\n","from keras.losses import categorical_crossentropy\n","\n","#To use adam optimizer for learning weights with learning rate = 0.001\n","optimizer = Adam(lr=0.001)\n","#Set the loss function and optimizer for the model training\n","model.compile(loss=categorical_crossentropy,\n","              optimizer=optimizer,\n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lCFcYHTm6-cE","colab":{"base_uri":"https://localhost:8080/","height":386},"outputId":"8bf45635-75b1-459e-9e04-3845c5f59eed","executionInfo":{"status":"ok","timestamp":1560685776800,"user_tz":-330,"elapsed":64388,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["#Training on the dataset\n","model.fit(x_train_2, y_train2_cat,\n","          batch_size=BATCH_SIZE,\n","          epochs=EPOCHS,\n","          verbose=1,\n","          validation_data=(x_test_2, y_test2_cat))"],"execution_count":139,"outputs":[{"output_type":"stream","text":["Train on 29404 samples, validate on 4861 samples\n","Epoch 1/10\n","29404/29404 [==============================] - 7s 226us/step - loss: 0.0134 - acc: 0.9956 - val_loss: 0.0248 - val_acc: 0.9949\n","Epoch 2/10\n","29404/29404 [==============================] - 6s 213us/step - loss: 0.0116 - acc: 0.9961 - val_loss: 0.0232 - val_acc: 0.9949\n","Epoch 3/10\n","29404/29404 [==============================] - 6s 214us/step - loss: 0.0104 - acc: 0.9965 - val_loss: 0.0226 - val_acc: 0.9953\n","Epoch 4/10\n","29404/29404 [==============================] - 6s 213us/step - loss: 0.0081 - acc: 0.9970 - val_loss: 0.0263 - val_acc: 0.9947\n","Epoch 5/10\n","29404/29404 [==============================] - 6s 212us/step - loss: 0.0087 - acc: 0.9967 - val_loss: 0.0218 - val_acc: 0.9953\n","Epoch 6/10\n","29404/29404 [==============================] - 6s 213us/step - loss: 0.0084 - acc: 0.9972 - val_loss: 0.0221 - val_acc: 0.9949\n","Epoch 7/10\n","29404/29404 [==============================] - 6s 214us/step - loss: 0.0066 - acc: 0.9977 - val_loss: 0.0222 - val_acc: 0.9957\n","Epoch 8/10\n","29404/29404 [==============================] - 6s 215us/step - loss: 0.0071 - acc: 0.9978 - val_loss: 0.0268 - val_acc: 0.9947\n","Epoch 9/10\n","29404/29404 [==============================] - 6s 213us/step - loss: 0.0072 - acc: 0.9974 - val_loss: 0.0236 - val_acc: 0.9957\n","Epoch 10/10\n","29404/29404 [==============================] - 6s 213us/step - loss: 0.0061 - acc: 0.9981 - val_loss: 0.0244 - val_acc: 0.9955\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f7fb93b88d0>"]},"metadata":{"tags":[]},"execution_count":139}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SoDozqghCJZ4"},"source":["## 12. Print the accuracy for classification of digits 5 to 9"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LRWizZIpCUKg","colab":{"base_uri":"https://localhost:8080/","height":67},"outputId":"cac2cf51-de5d-4a5b-ca20-4c59b4484a4c","executionInfo":{"status":"ok","timestamp":1560685836644,"user_tz":-330,"elapsed":3390,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["#Evaluating the model on train set\n","score = model.evaluate(x_train_2, y_train2_cat)\n","print('Train loss:', score[0])\n","print('Train accuracy:', score[1]*100,'%')"],"execution_count":142,"outputs":[{"output_type":"stream","text":["29404/29404 [==============================] - 3s 91us/step\n","Train loss: 0.0003386955433130696\n","Train accuracy: 99.99319820432594 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9fCxgb5s49Cj","colab":{"base_uri":"https://localhost:8080/","height":67},"outputId":"88188538-c61f-4e29-98c5-ef1102f34bb6","executionInfo":{"status":"ok","timestamp":1560685857168,"user_tz":-330,"elapsed":1274,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["#Testing the model on test set\n","score = model.evaluate(x_test_2, y_test2_cat)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1]*100,'%')"],"execution_count":144,"outputs":[{"output_type":"stream","text":["4861/4861 [==============================] - 0s 95us/step\n","Test loss: 0.024438329167176503\n","Test accuracy: 99.54741822670232 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FU-HwvIdH0M-"},"source":["## Sentiment analysis <br> \n","\n","The objective of the second problem is to perform Sentiment analysis from the tweets data collected from the users targeted at various mobile devices.\n","Based on the tweet posted by a user (text), we will classify if the sentiment of the user targeted at a particular mobile device is positive or not."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nAQDiZHRH0M_"},"source":["### 13. Read the dataset (tweets.csv) and drop the NA's while reading the dataset"]},{"cell_type":"code","metadata":{"id":"i0ar_FW733_f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"bb8c2e02-0a4a-4e87-8069-eaa9e95220fe","executionInfo":{"status":"ok","timestamp":1560686050138,"user_tz":-330,"elapsed":956,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["from google.colab import drive\n","drive.mount('/drive')"],"execution_count":145,"outputs":[{"output_type":"stream","text":["Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3eXGIe-SH0NA","colab":{}},"source":["data = pd.read_csv('/drive/My Drive/Colab Notebooks/data/tweets.csv', engine='python').dropna()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CWeWe1eJH0NF","colab":{"base_uri":"https://localhost:8080/","height":195},"outputId":"fa5dc6b6-2b1a-4d4e-b723-474b6c06376e","executionInfo":{"status":"ok","timestamp":1560688729672,"user_tz":-330,"elapsed":922,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["data.head()"],"execution_count":208,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_text</th>\n","      <th>emotion_in_tweet_is_directed_at</th>\n","      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n","      <td>iPhone</td>\n","      <td>Negative emotion</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n","      <td>iPad or iPhone App</td>\n","      <td>Positive emotion</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n","      <td>iPad</td>\n","      <td>Positive emotion</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>@sxsw I hope this year's festival isn't as cra...</td>\n","      <td>iPad or iPhone App</td>\n","      <td>Negative emotion</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n","      <td>Google</td>\n","      <td>Positive emotion</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          tweet_text  ... is_there_an_emotion_directed_at_a_brand_or_product\n","0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...  ...                                   Negative emotion\n","1  @jessedee Know about @fludapp ? Awesome iPad/i...  ...                                   Positive emotion\n","2  @swonderlin Can not wait for #iPad 2 also. The...  ...                                   Positive emotion\n","3  @sxsw I hope this year's festival isn't as cra...  ...                                   Negative emotion\n","4  @sxtxstate great stuff on Fri #SXSW: Marissa M...  ...                                   Positive emotion\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":208}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jPJvTjefH0NI"},"source":["### 14. Preprocess the text and add the preprocessed text in a column with name `text` in the dataframe."]},{"cell_type":"code","metadata":{"id":"Me_JjUVyKqiR","colab_type":"code","colab":{}},"source":["import spacy\n","import nltk\n","import re\n","nlp = spacy.load('en_core_web_sm')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5iec5s9gH0NI","colab":{}},"source":["#preprocessing using special character removal\n","def preprocess(text, remove_digits=False):\n","    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n","    text = re.sub(pattern, '', text)\n","    return text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EQSmqA-vH0NT","colab":{}},"source":["data['text'] = [preprocess(text) for text in data.tweet_text]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7kX-WoJDH0NV","colab":{"base_uri":"https://localhost:8080/","height":195},"outputId":"7ae5d826-d2e7-4510-96b2-c21ff51cb486","executionInfo":{"status":"ok","timestamp":1560688737498,"user_tz":-330,"elapsed":1333,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["data.head()"],"execution_count":212,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_text</th>\n","      <th>emotion_in_tweet_is_directed_at</th>\n","      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n","      <td>iPhone</td>\n","      <td>Negative emotion</td>\n","      <td>wesley83 I have a 3G iPhone After 3 hrs tweeti...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n","      <td>iPad or iPhone App</td>\n","      <td>Positive emotion</td>\n","      <td>jessedee Know about fludapp  Awesome iPadiPhon...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n","      <td>iPad</td>\n","      <td>Positive emotion</td>\n","      <td>swonderlin Can not wait for iPad 2 also They s...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>@sxsw I hope this year's festival isn't as cra...</td>\n","      <td>iPad or iPhone App</td>\n","      <td>Negative emotion</td>\n","      <td>sxsw I hope this years festival isnt as crashy...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n","      <td>Google</td>\n","      <td>Positive emotion</td>\n","      <td>sxtxstate great stuff on Fri SXSW Marissa Maye...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          tweet_text  ...                                               text\n","0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...  ...  wesley83 I have a 3G iPhone After 3 hrs tweeti...\n","1  @jessedee Know about @fludapp ? Awesome iPad/i...  ...  jessedee Know about fludapp  Awesome iPadiPhon...\n","2  @swonderlin Can not wait for #iPad 2 also. The...  ...  swonderlin Can not wait for iPad 2 also They s...\n","3  @sxsw I hope this year's festival isn't as cra...  ...  sxsw I hope this years festival isnt as crashy...\n","4  @sxtxstate great stuff on Fri #SXSW: Marissa M...  ...  sxtxstate great stuff on Fri SXSW Marissa Maye...\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":212}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OGWB3P2WH0NY"},"source":["### 15. Consider only rows having Positive emotion and Negative emotion and remove other rows from the dataframe."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bdgA_8N2H0NY","colab":{}},"source":["emotion = ['Negative emotion','Positive emotion']\n","data_filtered = data[data.is_there_an_emotion_directed_at_a_brand_or_product.isin(emotion)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_Jlu-reIH0Na","colab":{"base_uri":"https://localhost:8080/","height":67},"outputId":"332ebfc8-9bd0-4fa0-dc5c-789b34637f35","executionInfo":{"status":"ok","timestamp":1560688741938,"user_tz":-330,"elapsed":726,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["data_filtered.is_there_an_emotion_directed_at_a_brand_or_product.value_counts()"],"execution_count":214,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Positive emotion    2672\n","Negative emotion     519\n","Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"]},"metadata":{"tags":[]},"execution_count":214}]},{"cell_type":"code","metadata":{"id":"s56IMrUYQCN_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"66f7d7fa-4f1b-4587-dd2d-02fc9c4c868a","executionInfo":{"status":"ok","timestamp":1560688745815,"user_tz":-330,"elapsed":1203,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["data_filtered.shape"],"execution_count":215,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3191, 4)"]},"metadata":{"tags":[]},"execution_count":215}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SotCRvkDH0Nf"},"source":["### 16. Represent text as numerical data using `CountVectorizer` and get the document term frequency matrix\n","\n","#### Use `vect` as the variable name for initialising CountVectorizer."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YcbkY4sgH0Ng","colab":{}},"source":["# import and instantiate CountVectorizer (with the default parameters)\n","from sklearn.feature_extraction.text import CountVectorizer\n","vect = CountVectorizer(ngram_range=(1, 1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KyXtZGr-H0Nl","colab":{"base_uri":"https://localhost:8080/","height":118},"outputId":"60aab1a0-4553-4a89-c365-d19bb7177fd3","executionInfo":{"status":"ok","timestamp":1560688750607,"user_tz":-330,"elapsed":812,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["# learn the 'vocabulary' of the training data\n","vect.fit(data_filtered['text'])"],"execution_count":217,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n","                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n","                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n","                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n","                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                tokenizer=None, vocabulary=None)"]},"metadata":{"tags":[]},"execution_count":217}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aIdZYxJtH0Nq","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"91c632c0-b28e-44bb-bb58-42a72600a772","executionInfo":{"status":"ok","timestamp":1560688754279,"user_tz":-330,"elapsed":1406,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["# transform data into a 'document-term matrix'\n","data_filtered_dtm = vect.transform(data_filtered['text'])\n","data_filtered_dtm"],"execution_count":218,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<3191x6103 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 52449 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":218}]},{"cell_type":"code","metadata":{"id":"puYWTRxPN20p","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":244},"outputId":"671c61d4-ca7d-4f83-c358-d4ef503824e0","executionInfo":{"status":"ok","timestamp":1560688756341,"user_tz":-330,"elapsed":1369,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["# examine the vocabulary and document-term matrix together\n","pd.DataFrame(data_filtered_dtm.toarray(), columns=vect.get_feature_names()).head()"],"execution_count":219,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>02</th>\n","      <th>03</th>\n","      <th>0310</th>\n","      <th>10</th>\n","      <th>100</th>\n","      <th>1000</th>\n","      <th>10000</th>\n","      <th>100s</th>\n","      <th>100tc</th>\n","      <th>101</th>\n","      <th>1030</th>\n","      <th>1045am3</th>\n","      <th>105</th>\n","      <th>106</th>\n","      <th>10am</th>\n","      <th>10k</th>\n","      <th>10mins</th>\n","      <th>10pm</th>\n","      <th>10x</th>\n","      <th>11</th>\n","      <th>11m</th>\n","      <th>11ntc</th>\n","      <th>11th</th>\n","      <th>12</th>\n","      <th>1230p</th>\n","      <th>12b</th>\n","      <th>12th</th>\n","      <th>13</th>\n","      <th>130000</th>\n","      <th>136</th>\n","      <th>14</th>\n","      <th>140608</th>\n","      <th>1413</th>\n","      <th>1415</th>\n","      <th>14day</th>\n","      <th>15</th>\n","      <th>150</th>\n","      <th>1500</th>\n","      <th>150m</th>\n","      <th>157</th>\n","      <th>...</th>\n","      <th>york</th>\n","      <th>you</th>\n","      <th>you_</th>\n","      <th>youd</th>\n","      <th>youll</th>\n","      <th>youneedthis</th>\n","      <th>youquot</th>\n","      <th>your</th>\n","      <th>youre</th>\n","      <th>yours</th>\n","      <th>yourself</th>\n","      <th>youtube</th>\n","      <th>youve</th>\n","      <th>yowza</th>\n","      <th>yr</th>\n","      <th>yrs</th>\n","      <th>yrsday</th>\n","      <th>yummy</th>\n","      <th>yup</th>\n","      <th>zaarly</th>\n","      <th>zaarlyiscoming</th>\n","      <th>zagg</th>\n","      <th>zaggle</th>\n","      <th>zappos</th>\n","      <th>zazzle</th>\n","      <th>zazzlesxsw</th>\n","      <th>zazzlsxsw</th>\n","      <th>ze</th>\n","      <th>zelda</th>\n","      <th>zeldman</th>\n","      <th>zero</th>\n","      <th>zimride</th>\n","      <th>zip</th>\n","      <th>zite</th>\n","      <th>zms</th>\n","      <th>zombies</th>\n","      <th>zomg</th>\n","      <th>zone</th>\n","      <th>zoom</th>\n","      <th>zzzs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 6103 columns</p>\n","</div>"],"text/plain":["   02  03  0310  10  100  1000  ...  zms  zombies  zomg  zone  zoom  zzzs\n","0   0   0     0   0    0     0  ...    0        0     0     0     0     0\n","1   0   0     0   0    0     0  ...    0        0     0     0     0     0\n","2   0   0     0   0    0     0  ...    0        0     0     0     0     0\n","3   0   0     0   0    0     0  ...    0        0     0     0     0     0\n","4   0   0     0   0    0     0  ...    0        0     0     0     0     0\n","\n","[5 rows x 6103 columns]"]},"metadata":{"tags":[]},"execution_count":219}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5pxd5fSHH0Nt"},"source":["### 17. Find number of different words in vocabulary"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"p1DQ2LdNH0Nu","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"67fb48a9-0606-478d-f65d-146f50d4c8c8","executionInfo":{"status":"ok","timestamp":1560688761028,"user_tz":-330,"elapsed":1065,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["len(vect.get_feature_names())"],"execution_count":220,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6103"]},"metadata":{"tags":[]},"execution_count":220}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dwtgjTBeH0Ny"},"source":["#### Tip: To see all available functions for an Object use dir"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2n_iCcTNH0N0","colab":{"base_uri":"https://localhost:8080/","height":1294},"outputId":"cd9231cf-566e-426c-f779-d3f6bf38c2a9","executionInfo":{"status":"ok","timestamp":1560688764934,"user_tz":-330,"elapsed":1381,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["dir(vect)"],"execution_count":221,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['__class__',\n"," '__delattr__',\n"," '__dict__',\n"," '__dir__',\n"," '__doc__',\n"," '__eq__',\n"," '__format__',\n"," '__ge__',\n"," '__getattribute__',\n"," '__getstate__',\n"," '__gt__',\n"," '__hash__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__le__',\n"," '__lt__',\n"," '__module__',\n"," '__ne__',\n"," '__new__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__setattr__',\n"," '__setstate__',\n"," '__sizeof__',\n"," '__str__',\n"," '__subclasshook__',\n"," '__weakref__',\n"," '_char_ngrams',\n"," '_char_wb_ngrams',\n"," '_check_stop_words_consistency',\n"," '_check_vocabulary',\n"," '_count_vocab',\n"," '_get_param_names',\n"," '_get_tags',\n"," '_limit_features',\n"," '_more_tags',\n"," '_sort_features',\n"," '_stop_words_id',\n"," '_validate_custom_analyzer',\n"," '_validate_params',\n"," '_validate_vocabulary',\n"," '_white_spaces',\n"," '_word_ngrams',\n"," 'analyzer',\n"," 'binary',\n"," 'build_analyzer',\n"," 'build_preprocessor',\n"," 'build_tokenizer',\n"," 'decode',\n"," 'decode_error',\n"," 'dtype',\n"," 'encoding',\n"," 'fit',\n"," 'fit_transform',\n"," 'fixed_vocabulary_',\n"," 'get_feature_names',\n"," 'get_params',\n"," 'get_stop_words',\n"," 'input',\n"," 'inverse_transform',\n"," 'lowercase',\n"," 'max_df',\n"," 'max_features',\n"," 'min_df',\n"," 'ngram_range',\n"," 'preprocessor',\n"," 'set_params',\n"," 'stop_words',\n"," 'stop_words_',\n"," 'strip_accents',\n"," 'token_pattern',\n"," 'tokenizer',\n"," 'transform',\n"," 'vocabulary',\n"," 'vocabulary_']"]},"metadata":{"tags":[]},"execution_count":221}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ShA6D8jKH0N5"},"source":["### 18. Find out how many Positive and Negative emotions are there.\n","\n","Hint: Use value_counts on that column"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"q7LAl5pzH0N6","colab":{"base_uri":"https://localhost:8080/","height":67},"outputId":"436d3db8-6d77-43d5-c1b0-10aba82e473d","executionInfo":{"status":"ok","timestamp":1560688775592,"user_tz":-330,"elapsed":1815,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["data_filtered.is_there_an_emotion_directed_at_a_brand_or_product.value_counts()"],"execution_count":222,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Positive emotion    2672\n","Negative emotion     519\n","Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"]},"metadata":{"tags":[]},"execution_count":222}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IUvgj0FoH0N9"},"source":["### 19. Change the labels for Positive and Negative emotions as 1 and 0 respectively and store in a different column in the same dataframe named 'Label'\n","\n","Hint: use map on that column and give labels"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YftKwFv7H0N9","colab":{"base_uri":"https://localhost:8080/","height":118},"outputId":"f133a547-5f58-4b9d-b4f6-92e32cd71e75","executionInfo":{"status":"ok","timestamp":1560688781725,"user_tz":-330,"elapsed":1146,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["data_filtered['labels']=data_filtered.is_there_an_emotion_directed_at_a_brand_or_product.map({'Positive emotion':1,'Negative emotion':0})"],"execution_count":223,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"wZZ2m75VSnye","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":246},"outputId":"b763d2e3-a361-478b-c9ab-c3f4cb175c7e","executionInfo":{"status":"ok","timestamp":1560688786439,"user_tz":-330,"elapsed":1226,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["data_filtered.head()"],"execution_count":224,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_text</th>\n","      <th>emotion_in_tweet_is_directed_at</th>\n","      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n","      <th>text</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n","      <td>iPhone</td>\n","      <td>Negative emotion</td>\n","      <td>wesley83 I have a 3G iPhone After 3 hrs tweeti...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n","      <td>iPad or iPhone App</td>\n","      <td>Positive emotion</td>\n","      <td>jessedee Know about fludapp  Awesome iPadiPhon...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n","      <td>iPad</td>\n","      <td>Positive emotion</td>\n","      <td>swonderlin Can not wait for iPad 2 also They s...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>@sxsw I hope this year's festival isn't as cra...</td>\n","      <td>iPad or iPhone App</td>\n","      <td>Negative emotion</td>\n","      <td>sxsw I hope this years festival isnt as crashy...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n","      <td>Google</td>\n","      <td>Positive emotion</td>\n","      <td>sxtxstate great stuff on Fri SXSW Marissa Maye...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          tweet_text  ... labels\n","0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...  ...      0\n","1  @jessedee Know about @fludapp ? Awesome iPad/i...  ...      1\n","2  @swonderlin Can not wait for #iPad 2 also. The...  ...      1\n","3  @sxsw I hope this year's festival isn't as cra...  ...      0\n","4  @sxtxstate great stuff on Fri #SXSW: Marissa M...  ...      1\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":224}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3YErwYLCH0N_"},"source":["### 20. Define the feature set (independent variable or X) to be `text` column and `labels` as target (or dependent variable)  and divide into train and test datasets"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lNkwrGgEH0OA","colab":{}},"source":["X = data_filtered_dtm\n","Y = data_filtered['labels']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BmtWd0AdTqit","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"outputId":"b6e78dbd-e198-4d78-f05b-85b07da2937d","executionInfo":{"status":"ok","timestamp":1560689583114,"user_tz":-330,"elapsed":628,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["# spliting X and Y into training and testing sets\n","from sklearn.model_selection import train_test_split\n","X_train_dtm, X_test_dtm, y_train, y_test = train_test_split(X, Y,test_size=0.3, random_state=2)\n","print(X_train.shape)\n","print(X_test.shape)\n","print(y_train.shape)\n","print(y_test.shape)"],"execution_count":236,"outputs":[{"output_type":"stream","text":["(2233, 6103)\n","(958, 6103)\n","(2233,)\n","(958,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Q5nlCuaaH0OD"},"source":["## 21. **Predicting the sentiment:**\n","\n","\n","### Use Naive Bayes and Logistic Regression and their accuracy scores for predicting the sentiment of the given text"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2AbVYssaH0OE","colab":{}},"source":["# import and instantiate a Multinomial Naive Bayes model\n","from sklearn.naive_bayes import MultinomialNB\n","nb = MultinomialNB()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ktXrLhmOH0Of","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"99e112c9-53fb-4395-b2a2-24bc8fe85fdb","executionInfo":{"status":"ok","timestamp":1560689588119,"user_tz":-330,"elapsed":965,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["# train the model using X_train_dtm\n","nb.fit(X_train_dtm, y_train)"],"execution_count":237,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"]},"metadata":{"tags":[]},"execution_count":237}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"clv2X0kKH0Ok","colab":{}},"source":["# make class predictions for X_test_dtm\n","y_pred_class = nb.predict(X_test_dtm)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"K86LRMfdH0Ou","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"7d21eb30-960a-410a-f2e7-0bc69aa4920f","executionInfo":{"status":"ok","timestamp":1560689611393,"user_tz":-330,"elapsed":947,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["# calculate accuracy of class predictions\n","from sklearn import metrics\n","metrics.accuracy_score(y_test, y_pred_class)"],"execution_count":239,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8549060542797495"]},"metadata":{"tags":[]},"execution_count":239}]},{"cell_type":"code","metadata":{"id":"xd96tbYNWZ3X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"7c2803d0-5c99-41bf-f1be-fbe2d9ed8121","executionInfo":{"status":"ok","timestamp":1560689628521,"user_tz":-330,"elapsed":945,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["# print the confusion matrix\n","metrics.confusion_matrix(y_test, y_pred_class)"],"execution_count":240,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 54, 106],\n","       [ 33, 765]])"]},"metadata":{"tags":[]},"execution_count":240}]},{"cell_type":"code","metadata":{"id":"RF-OryYuWeLo","colab_type":"code","colab":{}},"source":["# import and instantiate a logistic regression model\n","from sklearn.linear_model import LogisticRegression\n","logreg = LogisticRegression()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQu16XLKWjLA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":3296},"outputId":"15c7736d-f961-4fe8-81d3-c6fa9a2ae2a9","executionInfo":{"status":"ok","timestamp":1560689685872,"user_tz":-330,"elapsed":972,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["# train the model using X_train_dtm\n","logreg.fit(X_train_dtm, y_train)\n","\n","# make class predictions for X_test_dtm\n","y_pred_class = logreg.predict(X_test_dtm)\n","\n","# calculate predicted probabilities for X_test_dtm\n","y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n","y_pred_prob"],"execution_count":242,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["array([0.98307648, 0.9668924 , 0.62424748, 0.09946203, 0.93559017,\n","       0.99518585, 0.8729795 , 0.99523776, 0.95224787, 0.9681021 ,\n","       0.96647662, 0.94484032, 0.9962137 , 0.87474799, 0.92885908,\n","       0.96922092, 0.05910354, 0.99361293, 0.96428779, 0.96066242,\n","       0.95170463, 0.65572119, 0.62268506, 0.97472789, 0.93218059,\n","       0.9838908 , 0.89329853, 0.99697541, 0.95933459, 0.92790573,\n","       0.96924184, 0.99842503, 0.93355818, 0.65001536, 0.73089328,\n","       0.95874044, 0.99614739, 0.74749049, 0.63646453, 0.9668828 ,\n","       0.97226285, 0.96361401, 0.77837282, 0.76379647, 0.81820739,\n","       0.2673044 , 0.9940449 , 0.69404547, 0.68960153, 0.9776847 ,\n","       0.99039984, 0.95717928, 0.99785671, 0.93141853, 0.90773494,\n","       0.99870981, 0.87672225, 0.93397907, 0.99405332, 0.98119378,\n","       0.98710668, 0.98535   , 0.99861685, 0.97608628, 0.79813603,\n","       0.89824111, 0.94784928, 0.9960485 , 0.93449542, 0.98070753,\n","       0.97232069, 0.79142647, 0.9110336 , 0.83385484, 0.99613319,\n","       0.96656359, 0.99415094, 0.6637819 , 0.89315524, 0.99421508,\n","       0.9968145 , 0.98768396, 0.98438015, 0.9908745 , 0.74366913,\n","       0.63735853, 0.97334094, 0.98253586, 0.74595322, 0.95643453,\n","       0.93036977, 0.94416953, 0.3853187 , 0.97734171, 0.77363352,\n","       0.85208262, 0.90691911, 0.970906  , 0.96358956, 0.79962675,\n","       0.96516437, 0.33090627, 0.98457128, 0.88210205, 0.9679126 ,\n","       0.95549178, 0.46975888, 0.94909883, 0.73154921, 0.99262473,\n","       0.98864017, 0.76845823, 0.95015191, 0.84288362, 0.99403423,\n","       0.98171926, 0.84155687, 0.76007784, 0.955269  , 0.10531168,\n","       0.92796252, 0.99130014, 0.77973091, 0.97958222, 0.98082275,\n","       0.96867071, 0.1453519 , 0.98712083, 0.93736157, 0.12737945,\n","       0.98697645, 0.98182026, 0.88731857, 0.9874097 , 0.99112798,\n","       0.89490351, 0.94732302, 0.97755582, 0.98419971, 0.98336987,\n","       0.59978012, 0.99192132, 0.65607947, 0.81632999, 0.54021405,\n","       0.3970624 , 0.99060562, 0.09850805, 0.97351661, 0.9803963 ,\n","       0.95621994, 0.84139298, 0.99732371, 0.98858215, 0.99170015,\n","       0.8396924 , 0.97206439, 0.78916838, 0.94252115, 0.94568305,\n","       0.91248042, 0.96067731, 0.87000509, 0.2377351 , 0.89171982,\n","       0.9872792 , 0.94733918, 0.98204184, 0.90310565, 0.76228163,\n","       0.95908253, 0.96365369, 0.7265624 , 0.91591903, 0.98293505,\n","       0.992927  , 0.90750422, 0.98660047, 0.97884024, 0.99226647,\n","       0.91445153, 0.99787772, 0.89747267, 0.98826141, 0.88992435,\n","       0.76464836, 0.93157943, 0.96852695, 0.76269413, 0.99896751,\n","       0.97474707, 0.95154539, 0.26583844, 0.68997396, 0.99928767,\n","       0.85381083, 0.02313485, 0.29679738, 0.31121674, 0.17449442,\n","       0.48577703, 0.81296044, 0.51087899, 0.92343757, 0.7095549 ,\n","       0.1651    , 0.98779104, 0.99722266, 0.92678646, 0.95055191,\n","       0.84718989, 0.93116105, 0.9865695 , 0.92659764, 0.9437397 ,\n","       0.97131133, 0.98306588, 0.76567535, 0.91619578, 0.88197619,\n","       0.97667959, 0.92402787, 0.98561437, 0.89547301, 0.92211019,\n","       0.98670621, 0.98920031, 0.9857974 , 0.98936816, 0.99452006,\n","       0.92050435, 0.98855577, 0.95854958, 0.93735535, 0.53408678,\n","       0.90903748, 0.99570972, 0.94102003, 0.98133104, 0.99574516,\n","       0.92728357, 0.89430883, 0.98484533, 0.99102331, 0.18981871,\n","       0.97217603, 0.99564772, 0.59367081, 0.78904649, 0.97575348,\n","       0.42491811, 0.96516692, 0.96221558, 0.98601471, 0.9722095 ,\n","       0.99443942, 0.99489463, 0.96666522, 0.92476251, 0.97735688,\n","       0.99296747, 0.97967248, 0.92590204, 0.95246101, 0.97352213,\n","       0.96842228, 0.99155787, 0.95668922, 0.31675983, 0.92619364,\n","       0.98913284, 0.98838892, 0.90966481, 0.96399384, 0.59692425,\n","       0.9850605 , 0.13471003, 0.99684771, 0.91997393, 0.92740356,\n","       0.53176   , 0.98547421, 0.80464726, 0.97668468, 0.48991977,\n","       0.98586971, 0.92353806, 0.93444776, 0.47868164, 0.9788023 ,\n","       0.99152851, 0.96619177, 0.99333053, 0.94729038, 0.99931397,\n","       0.80250449, 0.95392611, 0.97856278, 0.93094002, 0.96213489,\n","       0.94232619, 0.68624367, 0.26578335, 0.97228946, 0.98816575,\n","       0.9905802 , 0.80794281, 0.91009243, 0.99051049, 0.98696802,\n","       0.98691846, 0.97095662, 0.23855943, 0.9933238 , 0.95681083,\n","       0.94759651, 0.97577996, 0.85601587, 0.95593345, 0.98785838,\n","       0.9243161 , 0.99134914, 0.23016179, 0.89749614, 0.98089484,\n","       0.999057  , 0.84683164, 0.84531817, 0.99969892, 0.99777541,\n","       0.99100923, 0.93377137, 0.99016907, 0.83563301, 0.96761884,\n","       0.98883103, 0.98971573, 0.97899852, 0.76994949, 0.93866996,\n","       0.89732775, 0.97104395, 0.56219187, 0.93960555, 0.78264238,\n","       0.99460702, 0.98690564, 0.98609911, 0.98747228, 0.99784855,\n","       0.97657621, 0.96845971, 0.99696993, 0.93799107, 0.87936437,\n","       0.99124854, 0.91494795, 0.88413106, 0.88210493, 0.82484942,\n","       0.93090979, 0.17029627, 0.88203516, 0.97014107, 0.9158285 ,\n","       0.99005924, 0.9908375 , 0.69418743, 0.92678376, 0.16575319,\n","       0.40455097, 0.98965614, 0.98225846, 0.9953196 , 0.10145657,\n","       0.98702087, 0.97082321, 0.49676144, 0.97459602, 0.98629599,\n","       0.95647066, 0.99077315, 0.98145545, 0.76479452, 0.97607432,\n","       0.99299731, 0.94738527, 0.96989192, 0.95847023, 0.9481477 ,\n","       0.99598872, 0.85650351, 0.88539702, 0.99616721, 0.95172359,\n","       0.9089774 , 0.3097168 , 0.95245261, 0.99157709, 0.81410264,\n","       0.98974779, 0.98651977, 0.98371792, 0.83475405, 0.98806346,\n","       0.84477346, 0.96660843, 0.99853344, 0.94635142, 0.87676489,\n","       0.91650941, 0.94308128, 0.92755284, 0.94001928, 0.98822908,\n","       0.91892363, 0.99262139, 0.97521411, 0.59141357, 0.94679586,\n","       0.92474994, 0.75131049, 0.99850843, 0.98225923, 0.92809361,\n","       0.63341309, 0.99546487, 0.94266707, 0.94333334, 0.06202912,\n","       0.89804598, 0.98590793, 0.87566066, 0.93071058, 0.97665977,\n","       0.98292883, 0.58433496, 0.98873927, 0.96948803, 0.98009232,\n","       0.98613218, 0.74158472, 0.72324052, 0.99855917, 0.18437823,\n","       0.99864474, 0.82773115, 0.98729255, 0.85559087, 0.96064759,\n","       0.95927407, 0.87633993, 0.85598798, 0.97869805, 0.82384341,\n","       0.83390646, 0.97649513, 0.95635669, 0.79773778, 0.98823428,\n","       0.69024687, 0.98984515, 0.98102492, 0.19376474, 0.8401487 ,\n","       0.95768157, 0.90181588, 0.99481832, 0.94569935, 0.96732121,\n","       0.25173178, 0.9871458 , 0.93303387, 0.99494849, 0.53458477,\n","       0.95125485, 0.9922935 , 0.52950908, 0.4907442 , 0.97832736,\n","       0.95707345, 0.97612184, 0.29074147, 0.99085958, 0.97435075,\n","       0.97665977, 0.97868665, 0.76777919, 0.6593475 , 0.92285017,\n","       0.98181564, 0.90810615, 0.85733636, 0.98671355, 0.94597498,\n","       0.80610936, 0.48574347, 0.87107082, 0.88778687, 0.99377947,\n","       0.98996807, 0.96535779, 0.9746915 , 0.90864316, 0.97905626,\n","       0.97308899, 0.97356922, 0.5001375 , 0.98654942, 0.99435695,\n","       0.847574  , 0.97742052, 0.9537004 , 0.13085536, 0.96089085,\n","       0.08883927, 0.99590199, 0.57410734, 0.89360439, 0.74338981,\n","       0.87883061, 0.81131381, 0.96483687, 0.95022945, 0.98695535,\n","       0.99973764, 0.95327551, 0.99435994, 0.95553003, 0.43524159,\n","       0.96331151, 0.32682811, 0.9816038 , 0.17168416, 0.95934238,\n","       0.98801749, 0.91452452, 0.95130447, 0.58906458, 0.87448881,\n","       0.36015675, 0.97613539, 0.76758317, 0.98366888, 0.95632797,\n","       0.90274922, 0.77002264, 0.64925486, 0.97199908, 0.98094673,\n","       0.49213745, 0.96651497, 0.73823488, 0.993001  , 0.75141975,\n","       0.96378109, 0.89412171, 0.81716689, 0.99493463, 0.23728666,\n","       0.97833336, 0.93398637, 0.91502923, 0.96336625, 0.94046008,\n","       0.67424823, 0.95426457, 0.97109905, 0.99848818, 0.53977783,\n","       0.98650938, 0.89297823, 0.09891256, 0.26583844, 0.9735636 ,\n","       0.79780237, 0.99545478, 0.85015011, 0.99437331, 0.98828378,\n","       0.85300494, 0.9910667 , 0.6508659 , 0.92473049, 0.97996933,\n","       0.783864  , 0.80447656, 0.29624764, 0.14179268, 0.36238772,\n","       0.99317993, 0.811746  , 0.98119418, 0.96376656, 0.96584938,\n","       0.94646282, 0.53423946, 0.31822037, 0.84687983, 0.97545522,\n","       0.88148062, 0.9814649 , 0.49213853, 0.85916794, 0.98166916,\n","       0.92140242, 0.80061537, 0.96875706, 0.88333611, 0.94698879,\n","       0.61997847, 0.74809813, 0.9669595 , 0.97093513, 0.11639377,\n","       0.12197604, 0.99936801, 0.99387509, 0.46742577, 0.80680544,\n","       0.99754692, 0.95557258, 0.92738459, 0.91950598, 0.97966006,\n","       0.98976359, 0.49798498, 0.73819482, 0.94969111, 0.97835318,\n","       0.48022845, 0.96906098, 0.98940258, 0.63611323, 0.40379716,\n","       0.76758245, 0.99729839, 0.82872202, 0.81614284, 0.92152514,\n","       0.99457156, 0.70059587, 0.98421436, 0.99558919, 0.9276204 ,\n","       0.98798307, 0.27042045, 0.97285729, 0.94676153, 0.92248082,\n","       0.99475762, 0.34576302, 0.96384752, 0.8271716 , 0.97248813,\n","       0.99268112, 0.94835489, 0.98567413, 0.92837139, 0.96276206,\n","       0.99275268, 0.89220514, 0.99063216, 0.97460633, 0.95719523,\n","       0.76387511, 0.85951055, 0.84931239, 0.97278613, 0.55652922,\n","       0.74452843, 0.97273716, 0.91543462, 0.70444958, 0.95348838,\n","       0.32881021, 0.9245267 , 0.98631861, 0.88101807, 0.91045725,\n","       0.24903386, 0.97707198, 0.83185236, 0.78072241, 0.18551023,\n","       0.83004858, 0.9639703 , 0.54155867, 0.83736755, 0.96326292,\n","       0.58691779, 0.93613894, 0.97270331, 0.76686566, 0.99017494,\n","       0.82674908, 0.94918415, 0.99868637, 0.92769563, 0.99717641,\n","       0.28206774, 0.98009003, 0.88959132, 0.30765   , 0.88521856,\n","       0.98277803, 0.86627147, 0.97018504, 0.94662861, 0.38754848,\n","       0.94078369, 0.9800426 , 0.98859416, 0.98377742, 0.85363937,\n","       0.92975936, 0.9725481 , 0.99950052, 0.9448272 , 0.96869782,\n","       0.82507891, 0.95492679, 0.33244541, 0.96812553, 0.98989473,\n","       0.87514879, 0.7555472 , 0.98359984, 0.8088132 , 0.84172173,\n","       0.99098699, 0.87395289, 0.9530724 , 0.89726182, 0.54337943,\n","       0.71278647, 0.73293138, 0.99237072, 0.38480203, 0.96679043,\n","       0.91369206, 0.99351393, 0.42993231, 0.18281123, 0.99615909,\n","       0.9670297 , 0.99694263, 0.57458585, 0.93783744, 0.96136842,\n","       0.9722703 , 0.88075526, 0.98221492, 0.9646145 , 0.99440596,\n","       0.89170368, 0.66711567, 0.97921941, 0.952082  , 0.98002273,\n","       0.98750272, 0.92511219, 0.98530003, 0.8922863 , 0.888793  ,\n","       0.97801891, 0.98899754, 0.994062  , 0.99306109, 0.5058851 ,\n","       0.99280966, 0.99720208, 0.20236928, 0.92312445, 0.79808146,\n","       0.98253895, 0.90846928, 0.97743732, 0.97572146, 0.93084764,\n","       0.34234404, 0.96324857, 0.98550362, 0.88853198, 0.04123961,\n","       0.96744114, 0.97273737, 0.97463297, 0.78349189, 0.93427096,\n","       0.94784238, 0.97914614, 0.99330206, 0.73152148, 0.93238094,\n","       0.99575893, 0.92696628, 0.94924626, 0.87949811, 0.71751518,\n","       0.75217211, 0.96422448, 0.6023751 , 0.84820071, 0.27917286,\n","       0.95697399, 0.86982212, 0.92845214, 0.96865945, 0.94879708,\n","       0.77151565, 0.5568351 , 0.95315113, 0.85169757, 0.90287833,\n","       0.9852219 , 0.9985619 , 0.96872136, 0.93060707, 0.86756046,\n","       0.9820438 , 0.79260855, 0.98217938, 0.97235787, 0.57958848,\n","       0.99594971, 0.95360127, 0.10973714, 0.64520991, 0.95525855,\n","       0.05431174, 0.98875727, 0.65741934, 0.95313997, 0.97455787,\n","       0.98819011, 0.99154691, 0.87358809, 0.97053648, 0.44688352,\n","       0.99254196, 0.95502604, 0.87535231, 0.99164656, 0.807308  ,\n","       0.96141328, 0.99845833, 0.98430306, 0.99589814, 0.97208385,\n","       0.93227925, 0.81596427, 0.99254271, 0.99708466, 0.96597103,\n","       0.86458405, 0.9877621 , 0.70291684, 0.96270078, 0.98185576,\n","       0.5674332 , 0.93252937, 0.99622605, 0.15562522, 0.41759336,\n","       0.68650493, 0.90442086, 0.60298738, 0.98055549, 0.49902703,\n","       0.69262032, 0.98896231, 0.8776247 , 0.50973032, 0.86515099,\n","       0.98927332, 0.98696452, 0.74701205, 0.9639795 , 0.97487165,\n","       0.95414078, 0.97048361, 0.98857304, 0.06477208, 0.50801458,\n","       0.92605751, 0.46395877, 0.17890891, 0.92968517, 0.38281432,\n","       0.9982977 , 0.99595851, 0.14110161, 0.67804612, 0.99369528,\n","       0.91390593, 0.98197354, 0.73973635, 0.98692215, 0.91932941,\n","       0.73420582, 0.99421851, 0.99396768, 0.81715112, 0.98967577,\n","       0.91171446, 0.91330752, 0.93879014, 0.95926951, 0.97202605,\n","       0.97278478, 0.34894942, 0.99972818, 0.98771141, 0.24381687,\n","       0.98286408, 0.96763926, 0.61633585, 0.98416413, 0.13636042,\n","       0.82658655, 0.92479039, 0.43518466, 0.99925438, 0.69690844,\n","       0.98252991, 0.15535956, 0.54242051, 0.99503597, 0.91597872,\n","       0.57812443, 0.95236925, 0.99428091, 0.8671719 , 0.9435359 ,\n","       0.94246733, 0.79089225, 0.92777295, 0.98438827, 0.93702974,\n","       0.97973721, 0.08936935, 0.99384147, 0.71015845, 0.92472333,\n","       0.4026065 , 0.92400012, 0.97253611, 0.94599176, 0.99264267,\n","       0.99332412, 0.98122366, 0.98726948, 0.9980668 , 0.98680128,\n","       0.80884259, 0.90183552, 0.96677928])"]},"metadata":{"tags":[]},"execution_count":242}]},{"cell_type":"code","metadata":{"id":"ja4kLy4oWsrv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"1586d782-217f-4a28-cee2-a665e03470d2","executionInfo":{"status":"ok","timestamp":1560689714887,"user_tz":-330,"elapsed":1261,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["# calculate accuracy\n","metrics.accuracy_score(y_test, y_pred_class)"],"execution_count":243,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.872651356993737"]},"metadata":{"tags":[]},"execution_count":243}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sw-0B33tH0Ox"},"source":["## 22. Create a function called `tokenize_predict` which can take count vectorizer object as input and prints the accuracy for x (text) and y (labels)"]},{"cell_type":"code","metadata":{"id":"yoJxx3oAW2X8","colab_type":"code","colab":{}},"source":["X = data_filtered['text']\n","Y = data_filtered['labels']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dkcQGtDoXAFJ","colab_type":"code","colab":{}},"source":["x_train, x_test, y_train, y_test = train_test_split(X, Y,test_size=0.3, random_state=2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"okCTOs1TH0Oy","colab":{}},"source":["def tokenize_test(vect):\n","    x_train_dtm = vect.fit_transform(x_train)\n","    print('Features: ', x_train_dtm.shape[1])\n","    x_test_dtm = vect.transform(x_test)\n","    nb = MultinomialNB()\n","    nb.fit(x_train_dtm, y_train)\n","    y_pred_class = nb.predict(x_test_dtm)\n","    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TIYgjebGXJ0L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"284bc34e-c1d9-4537-d132-6d8b22e21c55","executionInfo":{"status":"ok","timestamp":1560689835307,"user_tz":-330,"elapsed":1000,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["tokenize_test(vect)"],"execution_count":247,"outputs":[{"output_type":"stream","text":["Features:  5066\n","Accuracy:  0.8653444676409185\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JxZ8jfPEH0O0"},"source":["### Create a count vectorizer function which includes n_grams = 1,2  and pass it to tokenize_predict function to print the accuracy score"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kdCyAN_IH0O0","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"d17e4e08-8d01-4a4c-e285-2a5b9df742d0","executionInfo":{"status":"ok","timestamp":1560689909927,"user_tz":-330,"elapsed":960,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["vect = CountVectorizer(ngram_range=(1, 2))\n","tokenize_test(vect)"],"execution_count":248,"outputs":[{"output_type":"stream","text":["Features:  23899\n","Accuracy:  0.8757828810020877\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"axepytmgH0O4"},"source":["### Create a count vectorizer function with stopwords = 'english'  and pass it to tokenize_predict function to print the accuracy score"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HToGkq7vH0O4","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"2240e47a-6795-48fb-9004-2fdbaf887ccf","executionInfo":{"status":"ok","timestamp":1560690001927,"user_tz":-330,"elapsed":897,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["vect = CountVectorizer(stop_words='english')\n","tokenize_test(vect)"],"execution_count":250,"outputs":[{"output_type":"stream","text":["Features:  4828\n","Accuracy:  0.8653444676409185\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iOIlJRxoH0O7"},"source":["### Create a count vectorizer function with stopwords = 'english' and max_features =300  and pass it to tokenize_predict function to print the accuracy score"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6fUhff-oH0O8","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"b567a08a-bff5-454d-c5b4-9c84da1bc190","executionInfo":{"status":"ok","timestamp":1560690024178,"user_tz":-330,"elapsed":1001,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["vect = CountVectorizer(stop_words='english',max_features=300)\n","tokenize_test(vect)"],"execution_count":251,"outputs":[{"output_type":"stream","text":["Features:  300\n","Accuracy:  0.8225469728601252\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"S2KZNWVkH0PA"},"source":["### Create a count vectorizer function with n_grams = 1,2  and max_features = 15000  and pass it to tokenize_predict function to print the accuracy score"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3v9XD082H0PB","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"bca3eee1-88c2-4cc6-d6ec-c098e601922c","executionInfo":{"status":"ok","timestamp":1560690068108,"user_tz":-330,"elapsed":941,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["vect = CountVectorizer(ngram_range=(1,2),max_features=15000)\n","tokenize_test(vect)"],"execution_count":253,"outputs":[{"output_type":"stream","text":["Features:  15000\n","Accuracy:  0.8716075156576201\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"We3JK_SRH0PO"},"source":["### Create a count vectorizer function with n_grams = 1,2  and include terms that appear at least 2 times (min_df = 2)  and pass it to tokenize_predict function to print the accuracy score"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fUHrfDCyH0PP","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"ddda4151-27a8-4e12-cfaa-cc8a7a9f70b0","executionInfo":{"status":"ok","timestamp":1560690090965,"user_tz":-330,"elapsed":935,"user":{"displayName":"Diksha Singh","photoUrl":"https://lh4.googleusercontent.com/-x3-ufhtCiQU/AAAAAAAAAAI/AAAAAAAAAFY/F5pCqY8v8Fg/s64/photo.jpg","userId":"07661533439928129655"}}},"source":["vect = CountVectorizer(ngram_range=(1,2),min_df=2)\n","tokenize_test(vect)"],"execution_count":254,"outputs":[{"output_type":"stream","text":["Features:  7325\n","Accuracy:  0.8768267223382046\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3H4k_lVZH0PS","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}